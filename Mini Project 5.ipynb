{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Project 5\n",
    "Abbaspour\n",
    "610398147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the biggest mini project ever!\n",
    "We're gonna train three word embedding models \"Word2Vec\", \"GloVe\" and \"FastText\" with first 1000 allocated documents for training these models\n",
    "and also Naive Bayes classifier and Support Vector Machine classifier.\n",
    "We'd like to do these with Latent Semantic Analysis (approximately, indeed!)\n",
    "First of all, as always, let's import essential libraries.\n",
    "The nltk attributes and punctuation from string library are imported for preprocessing the documents.\n",
    "listdir from os make us be accessed to data sets.\n",
    "distance from Levenshtein is used in finding and returning the most similar word exists in the vocabulary of any trained models.\n",
    "Library numpy attributes and crs_matrix from scipy.sparse are imported in order to handle arrayed operations and function.\n",
    "MultinomialNB from sklearn.naive_bayes is imported for creating and training Naive Bayes classifier.\n",
    "CountVectorizer and TfidfVectorizer from sklearn.feature_extraction.text are imported for TF vectorizing and TF-IDF vectorizing documents, respectively.\n",
    "TruncatedSVD from sklearn.decomposition is imported for Latent Semantic Analysis.\n",
    "SVC from sklearn.svm is imported for creating and training Support Vector Machine classifier.\n",
    "Whatever imported from sklearn.metrics is for machine learning methods evaluations.\n",
    "All imported attributes from matplotlib library have functions in plotting.\n",
    "All imported attributes from gensim library have functions in creating and training word embedding models for documents and words included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, itertools\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from Levenshtein import distance\n",
    "from numpy import array, add, zeros, transpose\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.pyplot import plot, subplots, box, show, scatter, title, xlabel, ylabel, annotate\n",
    "from matplotlib.colors import ListedColormap\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre proccesing function is now can only tokenize texts and removing some special useless characters included in data texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessor(String):\n",
    "    stopWords, String, Specials = stopwords.words('english'), wordpunct_tokenize(String), [\").\", \"\\'\", \",\", \";\", \".<\", \"br\", \"/><\", \"/>\"] # Case Folding & Tokenisations\n",
    "    return [Token.lower() for Token in String if Token not in punctuation and Token.lower() not in stopWords and Token not in Specials]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the most similar word exists in the vocabulary of any trained models. It handles key error in using the vector of words in documents which doesn't exist in vocabulary of any trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approx(unknownWord, Vocabulary):\n",
    "    # Listing similar words in the vocabulary to the unknown words.\n",
    "    listCounter = [distance(unknownWord, Word) for Word in Vocabulary]\n",
    "\n",
    "    # Retruning most similar word exists.\n",
    "    return Vocabulary[listCounter.index(min(listCounter))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is defined for SVD truncating the vectors of documents.\n",
    "The motivation of defining this function was to use it in the way of classify them with SVM, as we'll see, making document vectors and truncating them wouldn't be used.\n",
    "Despite description above, it's defined just in case.\n",
    "The documents topics are separated into two topics: \"Like\" and \"Dislike\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_doc_mat_LSA():\n",
    "    Topics = {0: \"Like\", 1: \"Disike\"}\n",
    "    LSA = LSI.fit_transform(tfidf.fit_transform(trainDocs))\n",
    "    print(f\"\\nSigma:\\n{LSI.singular_values_}\\n\") #Sigmas\n",
    "    print(f\"V^T:\\n{LSI.components_.T}\\n\")        #VT\n",
    "    Terms = tfidf.get_feature_names_out()\n",
    "    for Index, Component in enumerate(LSI.components_):\n",
    "        print(f\"{Topic[Index]}: {list(dict(sorted(zip(Terms, Component), key = lambda t: t[1], reverse = True)[:5]).keys())}\\n\")\n",
    "\n",
    "    print(LSI.explained_variance_ratio_)\n",
    "    plot(range(2), LSI.explained_variance_ratio_ * 100)\n",
    "    xlabel(\"Topic Number\")\n",
    "    ylabel(\"% explained\")\n",
    "    title(\"SVD dropoff\")\n",
    "    show()  # show first chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) This was defined for the prior fifth mini project which was truncated document vectors in any word embedding form by SVD in order to be used in Latent Semantic Analysis. The idea of making matrix of document vectors for SVD tranformation is from a paper attached to the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentLSA():\n",
    "    trainDocs_tf, trainDocs_tfidf, testDocs_tf = tfVectorizer.fit_transform(trainDocs), tfidfVectorizer.fit_transform(trainDocs), tfVectorizer.transform(testDocs)\n",
    "    tf_doc_term_matrix, tfidf_doc_term_matrix = trainDocs_tf.toarray(), trainDocs_tfidf.toarray() # Ordered document-term by tf.\n",
    "    sortedWords = sorted(list(set([Word for Sentence in Sentences for Word in Sentence])))[:tf_doc_term_matrix.shape[1]] # Dimension reduction for creating document vector matrix. (A logical error occures that latter words are not considered.)\n",
    "    term_doc_matrix, wordEmbeddings, validResponses = {\"TF\": \"tf term-document matrix\", \"IDF\": \"tf-idf term-document matrix\"}, {\"W2V\": \"Word2Vec\", \"GLV\": \"GloVe\", \"FT\": \"FastText\"}, [\"TF\", \"IDF\"]\n",
    "    while True:\n",
    "        term_doc_matrix_form = input(\"Tf term-document matrix or tf-idf term-document matrix? <Tf, Idf> \").upper()\n",
    "        if term_doc_matrix_form in validResponses:\n",
    "            print(f\"\\nYou chose {term_doc_matrix[term_doc_matrix_form]} matrix form.\")\n",
    "            break\n",
    "        print(\"\\nSelect one, dude!\")\n",
    "\n",
    "    validResponses = [\"W2V\", \"GLV\", \"FT\"]\n",
    "    while True:\n",
    "        word_embedding_technique = input(\"Word2Vec, GloVe or FastText? <W2V, GLV, FT> \").upper()\n",
    "        if word_embedding_technique in validResponses:\n",
    "            print(f\"\\nYou chose {wordEmbeddings[word_embedding_technique]} word embedding technique.\")\n",
    "            break\n",
    "        print(\"\\nSelect one, dude!\")\n",
    "\n",
    "    if term_doc_matrix_form == \"TF\":\n",
    "        # transpose(document-term matrix * word-embedded word vector matrix) = embedded word vector-document matrix.\n",
    "        if word_embedding_technique == \"W2V\": \n",
    "            return SVD.fit_transform(csr_matrix(transpose(tf_doc_term_matrix.dot(WordVec(Sentences)))))\n",
    "\n",
    "        elif word_embedding_technique == \"GLV\":\n",
    "            return SVD.fit_transform(csr_matrix(transpose(tf_doc_term_matrix.dot(GloVe(Sentences)))))\n",
    "            \n",
    "        return SVD.fit_transform(csr_matrix(transpose(tf_doc_term_matrix.dot(fastText(Sentences)))))\n",
    "\n",
    "    if word_embedding_technique == \"W2V\":\n",
    "        return SVD.fit_transform(csr_matrix(transpose(tfidf_doc_term_matrix.dot(WordVec(Sentences)))))\n",
    "\n",
    "    elif word_embedding_technique == \"GLV\":\n",
    "        return SVD.fit_transform(csr_matrix(transpose(tfidf_doc_term_matrix.dot(GloVe(Sentences)))))\n",
    "            \n",
    "    return SVD.fit_transform(csr_matrix(transpose(tfidf_doc_term_matrix.dot(fastText(Sentences)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used for achiving document vectors by means of Word2Vec word embedding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocVec():\n",
    "    taggedData = [TaggedDocument(words = trainDoc.split(), tags = [str(Tag)]) for Tag, trainDoc in enumerate(trainDocs)]\n",
    "    # train the Doc2vec model\n",
    "    Model = Doc2Vec(vector_size = 300, min_count = 2, epochs = 50)\n",
    "    Model.build_vocab(taggedData)\n",
    "    Model.train(taggedData, total_examples = Model.corpus_count, epochs = Model.epochs)\n",
    "\n",
    "    return [Model.infer_vector(trainDoc.split()) for trainDoc in trainDocs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for creating and training a model with one of two selective Word2Vec word embedding approaches \"Continuous Bag of Words\" and \"Skipgram\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordVec(Sentences):\n",
    "    validResponses = [\"CBOW\", \"SG\"]\n",
    "    while True:\n",
    "        modelMethod = input(\"CBOW or Skip-gram? <CBOW, Sg> \").upper()\n",
    "        if modelMethod in validResponses:\n",
    "            print(\"Alright!\")\n",
    "            break\n",
    "        print(\"Select one, dude!\")\n",
    "    \n",
    "    if modelMethod == \"CBOW\":\n",
    "        # Create CBOW model\n",
    "        Model = Word2Vec(min_count = 5, vector_size = 300, window = 5)\n",
    "    else:\n",
    "        # Create Skip-gram model(sg = 1)\n",
    "        Model = Word2Vec(min_count = 5, vector_size = 300, window = 5, sg = 1)\n",
    "\n",
    "    Model.build_vocab(Sentences, progress_per = 1000)\n",
    "\n",
    "    # train on our data\n",
    "    Model.train(Sentences, total_examples = len(Sentences), epochs = 100)\n",
    "\n",
    "    return Model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for creating and training a model with GloVe word embedding approach.\n",
    "It uses Stanford pretrained Glove model and then, update it with own dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GloVe(Sentences):\n",
    "    # Stanford pretrained\n",
    "    # Load GloVe embeddings using Gensim\n",
    "    gloVectors = KeyedVectors.load_word2vec_format('glove.6B.100d.txt', binary = False, no_header = True) # New version!\n",
    "\n",
    "    # build a toy model to update with\n",
    "    Model = Word2Vec(vector_size = 300, min_count = 5)\n",
    "    Model.build_vocab(Sentences)\n",
    "    numberExamples = Model.corpus_count\n",
    "\n",
    "    # add GloVe's vocabulary & weights.\n",
    "    Model.build_vocab([list(gloVectors.index_to_key)], update = True)\n",
    "\n",
    "    # train on our data\n",
    "    Model.train(Sentences, total_examples = numberExamples, epochs = Model.epochs)\n",
    "\n",
    "    return Model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for creating and training a model with FastText word embedding approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastText(Sentences):\n",
    "    phraseSentences = Phrases(Sentences, min_count = 30, progress_per = 10000)[Sentences]\n",
    "    Model = FastText(vector_size = 300, window = 5, min_count = 5, workers = 4, min_n = 1, max_n = 4)\n",
    "    \n",
    "    #Building Vocabulary\n",
    "    Model.build_vocab(phraseSentences)\n",
    "\n",
    "    # train on our data\n",
    "    Model.train(phraseSentences, total_examples = len(Sentences), epochs = 100)\n",
    "    \n",
    "    return Model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the function training SVM with or without Latent Semantic Analysis.\n",
    "Function 'Vectorizer' uses vectors of each word in any word embedding model and prepares them for SVM classifying.\n",
    "Function 'Suppvec' train Support Vector Machine classifier by means of word vectors by Vectorizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorizer(Sentence, WVs, LSA):\n",
    "    if LSA:\n",
    "        wordsVecs = LSI.fit_transform(csr_matrix([WVs[Word] for Word in Sentence if Word in WVs]))\n",
    "    else:\n",
    "        wordsVecs = [WVs[Word] for Word in Sentence if Word in WVs]\n",
    "        \n",
    "    if len(wordsVecs) == 0:\n",
    "        return zeros(300)\n",
    "    \n",
    "    wordsVecs = array(wordsVecs)\n",
    "    return wordsVecs.mean(axis = 0)\n",
    "\n",
    "def SuppVec(WVs, LSA):\n",
    "    transformedTrain = array([Vectorizer(Sentence, WVs, LSA) for Sentence in Sentences])\n",
    "    transformedTest = array([Vectorizer(Sentence, WVs, LSA) for Sentence in Sentences])\n",
    "\n",
    "    # Train a classification model\n",
    "    SVM.fit(transformedTrain, trainLabels)\n",
    "\n",
    "    predictLabels = SVM.predict(transformedTest)\n",
    "    print('Accuracy: ', accuracy_score(testLabels, predictLabels))\n",
    "    print('Precision: ', precision_score(testLabels, predictLabels, pos_label = 1))\n",
    "    print('Recall: ', recall_score(testLabels, predictLabels, pos_label = 1))\n",
    "    print('F1 score: ', f1_score(testLabels, predictLabels, pos_label = 1))\n",
    "    \n",
    "    # Generate the classification report.\n",
    "    print(classification_report(testLabels, predictLabels))\n",
    "\n",
    "    # Generate scatter plot for training data\n",
    "    scatter(transformedTrain[:,0], transformedTrain[:,1])\n",
    "    title('Linearly separable data')\n",
    "    xlabel('X1')\n",
    "    ylabel('X2')\n",
    "    show()\n",
    "    \n",
    "    # Get support vectors\n",
    "    supportVectors = SVM.support_vectors_\n",
    "\n",
    "    # Visualize support vectors\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(testLabels, predictLabels), display_labels = [\"Positive\", \"Negative\"]).plot()\n",
    "    scatter(transformedTrain[:,0], transformedTrain[:,1])\n",
    "    scatter(supportVectors[:,0], supportVectors[:,1], color = 'red')\n",
    "    title('Linearly separable data with support vectors')\n",
    "    xlabel('X1')\n",
    "    ylabel('X2')\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains Naive Bayes classifier with (OPTIONAL) or without TF-IDF vectorizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(tfidf):\n",
    "    # Training by Naive Bayes classifier method.\n",
    "    if tfidf:\n",
    "        # tf-idf vectorizing approach.\n",
    "        transformedTrain, transformedTest = tfVectorizer.fit_transform(trainDocs), tfVectorizer.transform(testDocs)\n",
    "    else:\n",
    "        vectorizerFit = tfVectorizer.fit(trainDocs)\n",
    "        transformedTrain, transformedTest = vectorizerFit.transform(trainDocs), vectorizerFit.transform(testDocs)\n",
    "\n",
    "    NBClassifier.fit(transformedTrain, trainLabels)\n",
    "\n",
    "    # Reporting accuracy.\n",
    "    predictLabels = NBClassifier.predict(transformedTest)\n",
    "    print(f\"Harmonic mean (F1 score) for Naive Bayes is {f1_score(testLabels, predictLabels)}\\n\")\n",
    "    print(f\"Accuracy score for Naive Bayes is {accuracy_score(testLabels, predictLabels)}\\n\")\n",
    "    \n",
    "    # Comprehensive classification report.\n",
    "    print(f\"Classification Report:\\n{classification_report(testLabels, predictLabels)}\")\n",
    "    ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(testLabels, predictLabels), display_labels = [\"Positive\", \"Negative\"]).plot()\n",
    "\n",
    "    # Plotting Precision-Recall Curve\n",
    "    Precision, Recall, Threshold = precision_recall_curve(testLabels, NBClassifier.predict_proba(transformedTest)[:,1])\n",
    "    fig, ax = subplots(figsize = (6,6))\n",
    "    ax.plot(Recall, Precision, label = \"Naive Bayes Classification\", color = \"firebrick\")\n",
    "    ax.set_title(\"Precision-Recall Curve\")\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    box(False)\n",
    "    ax.legend()\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's is allocating memories for some variables and creating preprocessed train and test documents with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals!\n",
    "tfVectorizer, tfidf, NBClassifier, wordEmbeddings, docEmbeddings, SVM, LSI, posTrain, negTrain, posTest, negTest, trainDocs, testDocs, trainLabels, testLabels, Sentences = CountVectorizer(), TfidfVectorizer(), MultinomialNB(), {WordVec: \"Word2Vec\", GloVe: \"GloVe\", fastText: \"FastText\"}, [\"DV\", \"WV\"], SVC(C = 1.0, kernel = 'linear', degree = 3, gamma = 'auto'), TruncatedSVD(n_components = 2, algorithm=\"arpack\"), listdir(\"train/pos\"), listdir(\"train/neg\"), listdir(\"test/pos\"), listdir(\"test/neg\"), [], [], [], [], []\n",
    "\n",
    "# Bound the number of trained and test data by 5000. Label '1' for positive and label '0' for negative.\n",
    "for Index in range(1000):\n",
    "    # Train data\n",
    "    Pos, Neg = preProcessor(open(f\"train/pos/{posTrain[Index]}\", \"r\").read()), preProcessor(open(f\"train/neg/{negTrain[Index]}\", \"r\").read())\n",
    "    Sentences += [Pos, Neg]\n",
    "\n",
    "    trainDocs += [\" \".join(Pos), \" \".join(Neg)]\n",
    "    trainLabels += [1, 0]\n",
    "\n",
    "    # Test data\n",
    "    testDocs += [\" \".join(preProcessor(open(f\"test/pos/{posTest[Index]}\", \"r\").read())), \" \".join(preProcessor(open(f\"test/neg/{negTest[Index]}\", \"r\").read()))]\n",
    "    testLabels += [1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling Latent Semantic Analysis term-document form and two forms of Naive Bayes classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start!\n",
    "print(\"A good approximation for term-document with tf-idf metric is:\")\n",
    "term_doc_mat_LSA()\n",
    "print(\"\\nHere's Naive Bayes classifiying results trained by train documents and labels and evaluated by being test on test documents and labels.\\n\")\n",
    "NB(False)\n",
    "print(\"And with tf-idf approach:\")\n",
    "NB(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create document vectors in two ways (Doc2Vec or summation of vectors of words in document respectively)(optional and useless so it became commented for proving that is made and made a requirement for this project) and check two forms of SVM classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wordEmbedding in wordEmbeddings: # Check evaluations for each wordembedding method\n",
    "    vectorWords, embeddingTechnique = wordEmbedding(Sentences), wordEmbeddings[wordEmbedding]\n",
    "    print(f\"\\nWith {embeddingTechnique} word embedding technique.\")\n",
    "    while True:\n",
    "        docEmbedding = input(\"\\nWanna transform documnets to vectors strictly or with word embedding vectors? <DV, WV> \").upper()\n",
    "        if docEmbedding in docEmbeddings:\n",
    "            print(\"\\nAlright\")\n",
    "            break\n",
    "        print(\"\\nChoose one, boy!\")\n",
    "\n",
    "    if docEmbedding == \"DV\":\n",
    "        vectorsDocs = DocVec()\n",
    "    else:\n",
    "        Vocabulary, Starter, vectorsDocs = list(vectorWords.key_to_index.keys()), array([0] * 300), []\n",
    "        for Document in Sentences:\n",
    "            documentVector = Starter\n",
    "            for Word in Document:\n",
    "                try:\n",
    "                    documentVector = add(documentVector, vectorWords[Word])\n",
    "                except KeyError:\n",
    "                    documentVector = add(documentVector, vectorWords[Approx(Word, Vocabulary)])\n",
    "\n",
    "            vectorsDocs.append(documentVector)\n",
    "\n",
    "    print(f\"\\nHere's Raw Support Vector Machine classifying with {embeddingTechnique} form embedded word vectors.\")\n",
    "    SuppVec(vectorWords, False)\n",
    "    print(f\"\\nAnd now, SVM according to Latent Semantic Analysis!\")\n",
    "    SuppVec(vectorWords, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations:\n",
    "According to classification report method, Accuracy, Precision, Recall, F1 score and Precision-Recall curve in both Naive Bayes approaches become the same.\n",
    "According to classification report method, Accuracy, Precision, Recall and F1 score in highest to lowest word embedding method order is:\n",
    "Word2Vec > FastText >> GloVe\n",
    "\n",
    "Advantages of Word2vec:\n",
    "The idea is very intuitive, which transforms the unlabled raw corpus into labeled data (by mapping the target word to its context word), and learns the representation of words in a classification task.\n",
    "The data can be fed into the model in an online way and needs little preprocessing, thus requires little memory.\n",
    "The mapping between the target word to its context word implicitly embeds the sub-linear relationship into the vector space of words, so that relationships like “king:man as queen:woman” can be infered by word vectors.\n",
    "It is simple for a freshman to understand the principle and do implementation.\n",
    "\n",
    "Disadvantages of Word2vec:\n",
    "The sub-linear relationships are not explicitly defined. There is little theoretical support behind such characteristic.\n",
    "The model could be very difficult to train if use the softmax function, since the number of categories is too large (the size of vocabulary). Though approxination algorithms like negative sampling (NEG) and hierarchical softmax (HS) are proposed to address the issue, other problems happen. For example, the word vectors by NEG are not distributed uniformally, they are located within a cone in the vector space hence the vector space is not sufficiently utilized.\n",
    "\n",
    "\n",
    "Advantages of Glove:\n",
    "The goal of Glove is very straightforward, i.e., to enforce the word vectors to capture sub-linear relationships in the vector space. Thus, it proves to perform better than Word2vec in the word analogoy tasks.\n",
    "Glove adds some more practical meaning into word vectors by considering the relationships between word pair and word pair rather than word and word.\n",
    "Glove gives lower weight for highly frequent word pairs so as to prevent the meaningless stop words like “the”, “an” will not dominate the training progress.\n",
    "\n",
    "Disadvantages of Glove:\n",
    "The model is trained on the co-occurrence matrix of words, which takes a lot of memory for storage. Especially, if you change the hyper-parameters related to the co-occurrence matrix, you have to reconstruct the matrix again, which is very time-consuming.\n",
    "\n",
    "Both Word2vec and Glove do not solve the problems like:\n",
    "How to learn the representation for out-of-vocabulary words.\n",
    "How to separate some opposite word pairs. For example, “good” and “bad” are usually located very close to each other in the vector space, which may limit the performance of word vectors in NLP tasks like sentiment analysis.\n",
    "\n",
    "FastText is generally considered faster than Word2vec because of its use of subword information. FastText represents words as bags of character n-grams, which allows it to capture information about morphology and subword units. This approach enables FastText to handle out-of-vocabulary words better than Word2vec and generally results in faster training times. However, the actual speed comparison can depend on various factors such as the size of the dataset, the specific implementation, and the hardware used for training.\n",
    "With any word embedding methods, applying Latent Semantic Analysis decreases evaluating metrics of SVM classification still in the same order above.\n",
    "(Probably due to using approximation and truncating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main challenge which still exists is how to use raw text files in continous operating and function calling being done without geeting error.\n",
    "Other challanges was training GloVe on own data and attribute errors from gensim library which has been upgraded from verison 3.0 to 4.0.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
